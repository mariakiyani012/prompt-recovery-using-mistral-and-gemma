{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7740846,"sourceType":"datasetVersion","datasetId":4524347},{"sourceId":7741042,"sourceType":"datasetVersion","datasetId":4524539},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":7907523,"sourceType":"datasetVersion","datasetId":4645060},{"sourceId":7933949,"sourceType":"datasetVersion","datasetId":4663472},{"sourceId":7970245,"sourceType":"datasetVersion","datasetId":4689677},{"sourceId":7989504,"sourceType":"datasetVersion","datasetId":4703298},{"sourceId":8049229,"sourceType":"datasetVersion","datasetId":4746640},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install ../input/hf-peft/peft-0.9.0-py3-none-any.whl\n%pip install ../input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\n%pip install ../input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-08-27T11:34:23.922763Z","iopub.execute_input":"2024-08-27T11:34:23.923157Z","iopub.status.idle":"2024-08-27T11:36:03.017595Z","shell.execute_reply.started":"2024-08-27T11:34:23.923124Z","shell.execute_reply":"2024-08-27T11:36:03.016370Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/hf-peft/peft-0.9.0-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (4.39.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.9.0) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.9.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.9.0) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.9.0) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.9.0) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.9.0) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.9.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.9.0) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.9.0) (1.3.0)\npeft is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\nProcessing /kaggle/input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\nbitsandbytes is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\nProcessing /kaggle/input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.2) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.2) (2024.2.2)\ntransformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:36:07.653065Z","iopub.execute_input":"2024-08-27T11:36:07.653453Z","iopub.status.idle":"2024-08-27T11:36:07.659249Z","shell.execute_reply.started":"2024-08-27T11:36:07.653420Z","shell.execute_reply":"2024-08-27T11:36:07.658154Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/llm-prompt-recovery/test.csv\")\n!cp ../input/llm-prompt-recovery/test.csv .","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:36:13.514110Z","iopub.execute_input":"2024-08-27T11:36:13.514482Z","iopub.status.idle":"2024-08-27T11:36:14.519779Z","shell.execute_reply.started":"2024-08-27T11:36:13.514452Z","shell.execute_reply":"2024-08-27T11:36:14.518601Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%writefile run.py\n\n# !cp ../input/recovery-scripts/run.py .\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nfrom peft import PeftModel, PeftConfig\nimport argparse\nimport numpy as np\n\n# Create the argument parser\nparser = argparse.ArgumentParser(description=\"\")\n\nparser.add_argument(\"--model_path\", type=str, help=\"\")\nparser.add_argument(\"--peft_path\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--model_type\", type=str, help=\"\")\nparser.add_argument(\"--prime\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--magic\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--output\", type=str, help=\"\")\nparser.add_argument(\"--max_len\", type=int, help=\"\")\nparser.add_argument(\"--min_output_len\", type=int, help=\"\", default=2)\nparser.add_argument(\"--max_output_len\", type=int, help=\"\", default=100)\nparser.add_argument('--quantize', action='store_true')\nparser.add_argument('--do_sample', action='store_true')\nparser.add_argument('--test_path', type=str)\n\nargs = parser.parse_args()\n\ntest = pd.read_csv(args.test_path)\nmagic = \"Transform the following text in a more vivid and descriptive way, while maintaining the original meaning and tone.\"\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\nlucrarea = args.magic\ndef predict_gemma(model, tokenizer, test, bad_words_ids=None):\n    if bad_words_ids is not None and len(bad_words_ids) == 0:\n        bad_words_ids = None\n    predictions = []\n    scores = []\n    with torch.no_grad():\n        for idx, row in tqdm(test.iterrows(), total=len(test)):\n            if row.original_text == row.rewritten_text:\n                predictions.append(\"Correct grammatical errors in this text.\")\n                continue\n            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n            prompt = f\"Find the orginal prompt that transformed original text to new text.\\n\\nOriginal text: {ot}\\n====\\nNew text: {rt}\"\n            conversation = [{\"role\": \"user\", \"content\": prompt }]\n            prime = args.prime\n            prompt = tokenizer.apply_chat_template(conversation, tokenize=False) + f\"<start_of_turn>model\\n{prime}\"\n            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, max_new_tokens=128, do_sample=args.do_sample, early_stopping=True, num_beams=1, bad_words_ids=bad_words_ids)\n            try:\n                x = tokenizer.decode(x[0]).split(\"<start_of_turn>model\")[1].split(\"<end_of_turn>\")[0].replace(\"<end_of_turn>\\n<eos>\",\"\").replace(\"<end_of_turn>\",\"\").replace(\"<start_of_turn>\",\"\").replace(\"<eos>\",\"\").replace(\"<bos>\",\"\").strip().replace('\"','').strip()\n                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\").replace(\"Revise\",\"Rewrite\")\n                x = x.split(\":\",1)[-1].strip()\n                if \"useruser\" in x:\n                    x = x.replace(\"user\",\"\")\n                if x[-1].isalnum():\n                    x += \".\"\n                else:\n                    x = x[:-1]+\".\"\n                x+= lucrarea\n                if len(x.split()) < args.max_output_len and len(x.split()) > args.min_output_len and (\"\\n\" not in x):\n                    print(x)\n                    predictions.append(x)\n                else:\n                    predictions.append(magic)\n            except Exception as e:\n                print(e)\n                predictions.append(magic)\n    return predictions\n\ndef predict_mistral(model, tokenizer, test,prime=\"\"):\n    predictions = []\n    with torch.no_grad():\n        for idx, row in tqdm(test.iterrows(), total=len(test)):\n            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n            prompt = f'''\nPlease find the prompt that was given to you to transform **original_text** to **new_text**. One clue is the prompt itself was short and concise.\nAnswer in thist format: \"It's likely that the prompt that transformed original_text to new_text was: <the prompt>\" and don't add anything else.\n\n**original_text**:\n{ot}\n\n**new_text**:\n{rt}\n'''\n            conversation = [{\"role\": \"user\", \"content\": prompt }]\n            prompt = tokenizer.apply_chat_template(conversation, tokenize=False)+prime\n            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=[13, tokenizer.eos_token_id], pad_token_id=tokenizer.eos_token_id, max_new_tokens=32, do_sample=args.do_sample, early_stopping=True, num_beams=1)\n            try:\n                x = tokenizer.decode(x[0]).split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip().split(\"\\n\",1)[0]\n                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\")\n                # print(x.split(\":\",1)[0])\n                x = x.split(\":\",1)[-1].strip()\n                if x[-1].isalnum():\n                    x += \".\"\n                else:\n                    x = x[:-1]+\".\"\n                x += lucrarea\n                if len(x.split()) < 50 and len(x.split()) > 2 and (\"\\n\" not in x):\n                    predictions.append(x)\n                else:\n                    predictions.append(magic)\n                print(predictions[-1])\n            except Exception as e:\n                print(e)\n                predictions.append(magic)\n    return predictions\nmodel_name = args.model_path\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbanned_ids = None\n    \nif args.quantize:\n    print(\"Use 4bit quantization\")\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                 quantization_config=quantization_config,\n                                                 device_map=\"auto\",\n                                                 torch_dtype=torch.bfloat16)\n    if args.peft_path != \"\":\n        print(\"Use peft\")\n        model = PeftModel.from_pretrained(model,\n                                    args.peft_path,\n                                    quantization_config=quantization_config,\n                                    torch_dtype=torch.bfloat16,\n                                    device_map=\"auto\")\nelse:\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                 device_map=\"auto\",\n                                                 torch_dtype=torch.bfloat16)\n    if args.peft_path != \"\":\n        print(\"Use peft\")\n        model = PeftModel.from_pretrained(model,\n                                args.peft_path,\n                                torch_dtype=torch.bfloat16,\n                                device_map=\"auto\")\n        \n# model = model.merge_and_unload()\nmodel.eval()\n# print(model)\nif args.model_type == \"gemma\":\n    preds = predict_gemma(model, tokenizer, test, bad_words_ids=banned_ids)\nelif args.model_type == \"mistral\":\n    preds = predict_mistral(model, tokenizer, test, prime=args.prime)\n\njson.dump(preds, open(args.output,\"wt\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:36:19.140951Z","iopub.execute_input":"2024-08-27T11:36:19.141345Z","iopub.status.idle":"2024-08-27T11:36:19.153040Z","shell.execute_reply.started":"2024-08-27T11:36:19.141312Z","shell.execute_reply":"2024-08-27T11:36:19.152132Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Overwriting run.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --peft_path \"../input/mistral-og-600\" --model_type \"mistral\" --output \"pred0.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Rewrite\" --magic \"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:39:38.673799Z","iopub.execute_input":"2024-08-27T11:39:38.674745Z","iopub.status.idle":"2024-08-27T11:42:09.020234Z","shell.execute_reply.started":"2024-08-27T11:39:38.674706Z","shell.execute_reply":"2024-08-27T11:42:09.019029Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Use 4bit quantization\nLoading checkpoint shards:   0%|                          | 0/2 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nLoading checkpoint shards: 100%|██████████████████| 2/2 [02:03<00:00, 61.81s/it]\nUse peft\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\nNo chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n2024-08-27 11:41:56.364713: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-27 11:41:56.364842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-27 11:41:56.491144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nRewrite the following text into a shanty.\n100%|█████████████████████████████████████████████| 1/1 [00:14<00:00, 14.14s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"!python run.py --model_path ../input/mistral-7b-it-v02/ --peft_path \"../input/mistral-gemmaonly\" --model_type \"mistral\" --output \"pred1.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Make this text\" --magic \"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:50:25.266686Z","iopub.execute_input":"2024-08-27T11:50:25.267716Z","iopub.status.idle":"2024-08-27T11:50:51.865053Z","shell.execute_reply.started":"2024-08-27T11:50:25.267676Z","shell.execute_reply":"2024-08-27T11:50:51.863962Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Use 4bit quantization\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:11<00:00,  3.85s/it]\nUse peft\n  0%|                                                     | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n2024-08-27 11:50:47.018991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-27 11:50:47.019048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-27 11:50:47.020584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nMake this text into a shanty about a code competition.\n100%|█████████████████████████████████████████████| 1/1 [00:04<00:00,  4.76s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"fns = [\"pred0.json\",\"pred1.json\"]\npreds = [json.load(open(x)) for x in fns]\npreds = [' '.join(list(x)) for x in zip(*preds)]\nprint(preds[:2])","metadata":{"execution":{"iopub.status.busy":"2024-08-27T11:53:15.762928Z","iopub.execute_input":"2024-08-27T11:53:15.763929Z","iopub.status.idle":"2024-08-27T11:53:15.770833Z","shell.execute_reply.started":"2024-08-27T11:53:15.763888Z","shell.execute_reply":"2024-08-27T11:53:15.769923Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['Rewrite the following text into a shanty. Make this text into a shanty about a code competition.']\n","output_type":"stream"}]}]}